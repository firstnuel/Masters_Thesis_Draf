\section{BACKGROUND}\label{chapter:lit-review}

\subsection{Log Anomaly Detection}

System log files, which are ubiquitously generated by networked computer systems, contain valuable information that is essential for monitoring system stability and security \cite{almodovar-etal-2022-language, DeepLog}. Logs describe detailed system events at runtime and chronologically record the intentions of users \cite{LogFormer}. As systems become increasingly complex, these logs serve as a crucial resource for fault diagnosis, performance monitoring, security auditing, and transaction tracing \cite{LogCTBL}.

The primary purpose of log anomaly detection is to protect digital infrastructures by identifying abnormal activities, such as network intrusions, from the enormous volumes of event logs. Anomalies can be defined as patterns in data that significantly deviate from the expected behavior of the system. Detecting these deviations is crucial for maintaining system reliability and preventing severe disruptions or financial losses, as global cybercrime costs are estimated to reach trillions of euros annually \cite{europeanparliament2024cybercrime}.

Log analysis, and consequently anomaly detection, faces several significant challenges primarily driven by the nature and scale of the data:

\begin{itemize}
     \item \textbf{Volume:} System logs are large-scale data collected in real-time \cite{LAnoBERT}. The sheer volume of logs has grown rapidly, often reaching 50 GB (120–200 million lines) per hour for large-scale services, making manual inspection and traditional processing infeasible \cite{LogFormer}.

    \item \textbf{Variety and Complexity:} Logs are typically unstructured or semi-structured text files generated by logging statements in source code \cite{DeepLog}. Because developers are allowed to write free-text messages, the format and semantics of logs vary significantly from system to system, leading to high-dimensional features with complex interrelationships. This complexity and diversity increase the difficulty of accurate anomaly detection \cite{Drain}.

    \item \textbf{Velocity (Timeliness):} For anomaly detection to be useful, it must be timely, requiring decisions to be made in a streaming fashion to allow users to intervene in ongoing attacks or performance issues. Offline methods that require multiple passes over the entire log data are thus unsuitable for real-time security monitoring \cite{DeepLog}.
    
\end{itemize}

Due to the challenges of volume and complexity, 
the adoption of automated log analysis has become imperative to efficiently process and interpret vast corpora of logs.

\subsection{Traditional Methods}
Early log anomaly detection efforts relied heavily on human expertise \cite{LogCTBL}. As the volume of logs grew, research shifted toward automated, data-driven methods, broadly categorized into rule-based systems and statistical approaches, many of which depend on logs first being converted into a structured format through a process known as log parsing \cite{Drain}.

Log parsing is a critical precursor step where raw, unstructured log messages are transformed into structured data, typically by extracting a constant part, called the log template (or log key), and identifying the variable parts (parameters).
The parser Spell, for example, is an online streaming parser that utilizes the Longest Common Subsequence (LCS) technique to dynamically identify and update log patterns. Tools like DeepLog rely on log parsing methods like Spell to generate log templates for their inputs \cite{DeepLog, Spell}.

\subsubsection{Rule-Based Systems (Regex, Signatures)}
Rule-based methodologies were among the first attempts to automate log analysis to reduce human error. These methods typically rely on explicitly defined rules, patterns, or known indicators of abnormal behavior, often requiring specific domain knowledge from human experts.

\begin{itemize}
    \item \textbf{Keyword Matching and Regular Expressions (Regex):} Early rule-based systems focused on matching specific keywords (e.g., “error,” “failed”) or using regular expressions to flag anomalous log entries \cite{robust}. However, relying solely on keywords or structural features often prevents a large portion of log anomalies from being detected and can lead to unnecessary alarms (alarm fatigue) if the system constantly evolves \cite{robust,LogAnomaly}. Furthermore, manually designing and maintaining regular expressions is prohibitive given the rapid increase in log volume and frequent system updates \cite{Drain}.

    \item \textbf{Invariant Mining (IM):} Invariant mining is another traditional approach that captures co-occurrence patterns between different log keys \cite{DeepLog,LogAnomaly}. This method defines a window (time or session based) and detects whether certain mined quantitative relationships, or invariants, hold true within that window (e.g., ensuring that the count of “file open” logs equals the count of “file close” logs in a normal condition) \cite{LogAnomaly}. IM is typically characterized as an unsupervised offline method \cite{DeepLog}.
\end{itemize}

\subsubsection{Statistical Methods (Clustering, PCA)}
Statistical methods leverage mathematical principles to identify normal patterns from data volumes, and flag deviations statistically likely to be anomalous. These methods generally operate on a generated numeric vector representation of the logs, often discarding parameter values and only using log keys and their counts \cite{DeepLog}. Most statistical methods rely on initial log parsing, where raw log messages are converted into structured, numeric representations, such as event count vectors \cite{SystemLog}.

\begin{itemize}
    \item \textbf{Principal Component Analysis (PCA):} PCA is a widely used statistical method for log anomaly detection. PCA is a linear transformation technique used to convert a set of correlated variables into a set of uncorrelated variables, known as principal components \cite{LAnoBERT, geron2019hands}.
    \item 
\end{itemize}
