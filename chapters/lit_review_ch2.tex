\section{Background}\label{chapter:lit-review}

\subsection{Log-Based Anomaly Detection}

\subsubsection{Overview and Challenges}

Log files contain fundamental information for monitoring the stability and security of networked systems; the logs are generated ubiquitously \cite{almodovar-etal-2022-language, DeepLog}. Logs are valuable because they provide detailed and chronological records of system events, which can include warnings, errors, or information on system changes, as well as user intentions.\cite{aws_log_files, LogFormer}. As system complexity continues to grow, these logs have become a very important asset for operations such as performance monitoring, security auditing, transaction tracing, and fault diagnosis \cite{LogCTBL}. The primary purpose of log anomaly detection is to protect digital infrastructures by identifying abnormal activities, such as network intrusions, from the enormous volumes of event logs. In this context, anomalies represent log patterns that significantly deviate from the expected behavior of the system. Detecting these deviations is crucial for maintaining system reliability and preventing severe disruptions or financial losses, as global cybercrime costs are estimated to reach trillions of euros annually \cite{europeanparliament2024cybercrime}.

Log analysis, and consequently anomaly detection, faces several significant challenges primarily driven by the nature and scale of the data:

\begin{itemize}

     \item \textbf{Volume:} System logs are large-scale data collected in real-time \cite{LAnoBERT}. The sheer volume of logs has grown rapidly, often reaching 50 GB (120–200 million lines) per hour for large-scale services, making manual inspection and traditional processing infeasible \cite{LogFormer}.

    \item \textbf{Variety and Complexity:} Logs are typically unstructured or semi-structured text files generated by logging statements in source code \cite{DeepLog}. Because developers are allowed to write free-text messages, the format and semantics of logs vary significantly across systems, leading to high-dimensional features with complex interrelationships. This complexity and diversity increase the difficulty of accurate anomaly detection \cite{Drain}.

    \item \textbf{Velocity (Timeliness):} For anomaly detection to be useful, it must be timely, requiring decisions to be made in a streaming fashion to allow users to intervene in ongoing attacks or performance issues. Offline methods that require multiple passes over the entire log data are unsuitable for real-time security monitoring \cite{DeepLog}.

\end{itemize}

Due to the challenges of volume and complexity, the adoption of automated log analysis has become imperative to efficiently process and interpret vast corpora of logs.


\subsubsection{Traditional Methods}

Early log anomaly detection efforts relied heavily on human expertise \cite{LogCTBL}. As the volume of logs grew, research shifted toward automated, data-driven methods, broadly categorized into rule-based systems and statistical approaches, many of which depend on logs first being converted into a structured format through a process known as log parsing \cite{Drain}. Log parsing is a critical precursor step where raw, unstructured log messages are transformed into structured data, typically by extracting a constant part, called the log template (or log key), and identifying the variable parts (parameters). The parser Spell, for example, is an online streaming parser that utilizes the Longest Common Subsequence (LCS) technique to dynamically identify and update log patterns. Tools like DeepLog rely on log parsing methods like Spell to generate log templates for their inputs \cite{DeepLog, Spell}.

\paragraph{Rule-Based Systems (Regex, Signatures)}

Rule-based methodologies were among the first attempts to automate log analysis to reduce human error. These methods typically rely on explicitly defined rules, patterns, or known indicators of abnormal behavior, often requiring specific domain knowledge from human experts.

\begin{itemize}

    \item \textbf{Keyword Matching and Regular Expressions (Regex):} Early rule-based systems focused on matching specific keywords (e.g., "error," "failed") or using regular expressions to flag anomalous log entries \cite{robust}. However, relying solely on keywords or structural features often prevents a large portion of log anomalies from being detected and can lead to unnecessary alarms (alarm fatigue) if the system constantly evolves \cite{robust,LogAnomaly}. Furthermore, manually designing and maintaining regular expressions is prohibitive given the rapid increase in log volume and frequent system updates \cite{Drain}.

    \item \textbf{Invariant Mining (IM):} Invariant mining is another traditional approach that captures co-occurrence patterns between different log keys \cite{DeepLog,LogAnomaly}. This method defines a window (time or session based) and detects whether certain mined quantitative relationships, or invariants, hold true within that window (e.g., ensuring that the count of "file open" logs equals the count of "file close" logs in a normal condition) \cite{LogAnomaly}. IM is typically characterized as an unsupervised offline method \cite{DeepLog}.

\end{itemize}


\paragraph{Statistical Methods (Clustering, PCA)}

Statistical methods leverage mathematical principles to identify normal patterns and flag deviations statistically likely to be anomalous. These methods typically operate on numeric vector representations of logs, often using only log keys and their counts rather than parameter values \cite{DeepLog}. Most statistical methods rely on initial log parsing to convert raw log messages into structured, numeric representations such as event count vectors \cite{SystemLog}.

\begin{itemize}
    \item \textbf{Principal Component Analysis (PCA):} PCA is a linear transformation technique that converts correlated variables into uncorrelated principal components \cite{LAnoBERT, geron2019hands}. In log analysis, PCA operates at the session level, where log entries are grouped by an identifier (e.g., block\_id in HDFS) and converted into event count vectors recording log key frequencies \cite{SystemLog, DeepLog}. PCA projects this high-dimensional counting matrix into a lower-dimensional space by identifying components capturing the most variance \cite{LogCTBL}. It produces a normal space ($S_n$) from the first $k$ principal components and an anomaly space ($S_a$) from the remaining dimensions \cite{SystemLog}. Anomalies are detected by measuring a session's projection length onto $S_a$ using the Squared Prediction Error (SPE) \cite{SystemLog, robust, DeepLog}.

    \item \textbf{Clustering Methods:} Clustering algorithms group similar data instances to identify patterns or outliers \cite{alammar2024hands, geron2019hands}. LogCluster is an unsupervised method that groups textually similar log messages using IDF-scaled vectorization to build a knowledge base of normal and abnormal clusters \cite{SystemLog, LogCTBL}. Sequences are classified as anomalous if their distance to the nearest cluster centroid exceeds a threshold \cite{LogCTBL}. Density-based approaches like SLCT group frequently occurring messages into clusters representing common patterns, treating entries outside these clusters as potential anomalies \cite{LogHound}. HDBSCAN extends this approach and is sometimes used to provide pseudo-labels for semi-supervised learning \cite{LogCTBL}.


\end{itemize}




\subsubsection{Machine Learning Approaches}

Following traditional statistical and rule-based methods, log anomaly detection quickly adopted general Machine Learning (ML) algorithms. These approaches rely heavily on the preceding log parsing and vectorization steps to transform unstructured log files into numerical features \cite{almodovar-etal-2022-language}. Traditional ML techniques are generally categorized into supervised models, which require labeled data, and unsupervised models, which are crucial given that log data is overwhelmingly unlabeled in real-world scenarios \cite{robust,LAnoBERT}.

\paragraph{Classical Methods}

Traditional machine learning algorithms require structured, vectorized representations of logs, typically event count matrices derived from parsed log templates. Support Vector Machines (SVM) is a flexible model capable of classification and outlier detection. One-Class SVM (OCSVM) variant is trained only on normal data to identify anomalies that fall outside a learned boundary \cite{oneclasssvm,geron2019hands}. Random Forests (RF) is an ensemble method that builds multiple decision trees on random subsets of features and data. RF models are effective with minimal parameter tuning and often outperform other classifiers in log anomaly detection \cite{muller2016introduction,ali2025}. Isolation Forest (iForest) is a tree-based unsupervised algorithm that isolates observations distinct from the remaining data. Unlike clustering methods that seek dense regions, iForest forms an ensemble of decision trees where outliers are isolated more quickly than normal instances \cite{IsolationForest,LAnoBERT,geron2019hands}.


\subsubsection{Deep Learning Models}

Deep learning approaches use neural networks to automatically learn representations from log sequences, treating logs analogously to natural language. These models typically follow a pipeline of preprocessing, parsing, vectorization, and neural network classification \cite{hashemi2025}.

\paragraph{LSTM-based Models (DeepLog)}

DeepLog, proposed by Du et al. (2017), uses Long Short-Term Memory networks to learn normal execution patterns through a forecasting-based self-supervised approach. The model predicts the probability distribution of the next log key given a history of preceding keys derived from parsing tools like Spell \cite{DeepLog,Spell,robust}. An incoming log key is classified as normal if it falls within the top $g$ candidates with the highest predicted probabilities, enabling detection at the granular per-log-entry level \cite{DeepLog,LogCTBL}.

\paragraph{Transformer-based Models (LogBERT)}

LogBERT, proposed by Guo et al. (2021), applies Bidirectional Encoder Representations from Transformers (BERT) to log anomaly detection. It encodes each log key using bidirectional self-attention, capturing dependencies across entire sequences \cite{LogBERT}. LogBERT is trained solely on normal log data using two self-supervised objectives: Masked Log Key Prediction (MLKP), which predicts masked log keys to learn normal patterns, and Volume of Hypersphere Minimization (VHM), which encourages embeddings of normal sequences to cluster closely in latent space \cite{LogCTBL,LAnoBERT,LogBERT}. This approach consistently outperforms LSTM-based methods due to BERT's stronger contextual understanding \cite{LAnoBERT}.

\paragraph{Attention Mechanisms}

Attention mechanisms, introduced by Vaswani et al. (2017) in \textit{"Attention Is All You Need"}, enable models to dynamically weigh the importance of different elements within input sequences \cite{AttentionIsAllYouNeed}. In Transformer models like LogBERT, multi-head self-attention enables each log key to attend to every other key, producing context-aware embeddings that capture semantic and sequential relationships \cite{LogBERT}. Beyond Transformers, attention has been integrated into LSTM-based models like LogRobust for improved robustness, and hybrid architectures like LogCTBL that combine convolutional, recurrent, and attention layers \cite{robust,LogCTBL}. Attention weights can also provide interpretability by revealing which tokens influenced predictions, though their reliability as explanatory tools remains debated \cite{AttentionNotExplanation}.

Deep learning models consistently outperform traditional statistical methods and classical ML approaches, with superior F1 scores attributed to their ability to learn semantic embeddings and model complex sequential structures \cite{almodovar-etal-2022-language}. Table X compares representative models across benchmark datasets.

\begin{table}[h!]
\centering
\small
\begin{tabular}{|l|p{4.2cm}|p{2cm}|c|c|p{1.3cm}|}
\hline
\textbf{Model} & \textbf{Architecture Type} & \textbf{Log Parser Dep.} & \textbf{HDFS F1} & \textbf{BGL F1} & \textbf{T-bird F1} \\
\hline
LogCTBL \cite{LogCTBL} & Hybrid (CNN-TCN-Bi-LSTM + BERT) & Yes (Drain3) & - & 0.9987 & 0.9978 \\
LAnoBERT \cite{LAnoBERT} & Transformer (BERT-MLM, Parser-free) & No & 0.9645 & 0.8749 & 0.9990 \\
OneLog \cite{hashemi2025} & End-to-End HCNN (Character-based) & No & 0.9900 & 0.9900 & 0.9900 \\
LogFiT \cite{almodovar-etal-2022-language} & Transformer (Fine-tuned BERT) & No & 0.9497 & 0.9122 & 0.9414 \\
DeepLog \cite{DeepLog} & LSTM-based (Forecasting) & Yes (Spell) & 0.7734 & 0.8612 & 0.9308 \\
LogBERT \cite{LogBERT} & Transformer (BERT-MLKP/VHM) & Yes (Drain) & 0.8232 & 0.9083 & 0.9664 \\
LogFormer \cite{LogFormer} & Transformer (Pre-train/Tuning) & Yes (Drain) & 0.9800 & 0.9700 & 0.9900 \\
\hline
\end{tabular}
\caption{Comparison of representative log anomaly detection models.}
\end{table}


\subsection{Explainable AI (XAI) Techniques}

Explainable Artificial Intelligence (XAI) encompasses methods that help users interpret and understand how machine learning models make their decisions, thereby improving transparency \cite{XAImersha2024,hashemi2025}. The goal of XAI is to build comprehension regarding the influences on a model, how that influence occurs, and where the model succeeds or fails \cite{XAImersha2024}. The need for explainability is paramount in high-stake security applications such as anomaly detection, vulnerability detection, and malware detection \cite{SoKBhusal_2023,hashemi2025}. Deep learning models, while achieving high accuracy in security domains, often function as black boxes, obscuring the features and factors that lead to their predictions. This opacity leads to uncertainty and distrust, especially when an incorrect prediction carries severe consequences \cite{SoKBhusal_2023}.

A fundamental challenge in implementing AI, particularly in high-stakes domains, is managing the tension between model accuracy and interpretability \cite{Unified}. Deep learning models typically achieve the highest accuracy on complex, large datasets, but their complexity makes their decisions difficult to understand \cite{Unified}. Conversely, highly interpretable models, such as decision trees, may be easy for humans to understand but often struggle to match the performance of complex black-box models, especially when dealing with unstructured raw data like system logs \cite{SoKBhusal_2023}. Consequently, achieving a usable XAI solution requires finding a balance where the interpretability gained justifies the complexity introduced. Developers must choose between designing intrinsically interpretable models (ante-hoc explanation) or applying post-hoc explanation methods to analyze the decisions of highly accurate black-box models.

\subsubsection{SHAP and Attention Mechanisms}


SHapley Additive exPlanations (SHAP) is a post-hoc method that generates local explanations for the predictions of any ML model \cite{SoKBhusal_2023} by viewing the explanation process through the lens of game theory, specifically utilizing Shapley values \cite{Unified}. SHAP introduces a group of methods that explain a model’s prediction by adding up the effects of each feature, in which the explanation model is a linear function of binary variables indicating feature presence or absence \cite{XAImersha2024, Unified}. SHAP values uniquely satisfy desirable properties: local accuracy (the explanation model matches the original prediction), missingness (features missing in the input have no impact), and consistency. SHAP assigns an importance value (the Shapley value) to each feature, quantifying its contribution to the model's output by averaging its marginal contribution across all possible subsets of other features \cite{Unified}. Although SHAP is computationally slower than LIME, its foundation in game-theoretic principles generally leads to more robust explanations \cite{SoKBhusal_2023}. Deep SHAP is a model-specific approximation method designed to leverage the compositional nature of deep networks to rapidly estimate SHAP values \cite{Unified}.


Transformer models rely on attention mechanisms that allow models to selectively focus on different parts of the input sequence to compute representations \cite{SoKBhusal_2023,XAImersha2024}. The attention function maps a query and a set of key-value pairs to an output, calculated as a weighted sum of values, where weights are derived from the compatibility score between the query and corresponding key \cite{AttentionIsAllYouNeed,geron2019hands}. These attention weights can be leveraged as a form of explanation by quantifying the importance assigned by the model. When used for explainability, the magnitude of the attention weight assigned to a specific token or segment indicates its perceived significance to the model's prediction \cite{XAImersha2024}.


 Several tools have been developed to map attention scores back onto input data. BERTViz \cite{BertViz} is an open-source tool for visualizing multi-head self-attention in BERT-based models, providing views at the attention-head, model, and neuron levels \cite{AnomalyExplainer,BertViz}. Other visualizations, such as Attention-Viz, highlight relationships between query and key embeddings for global analysis of patterns across sequences \cite{AttentionIsAllYouNeed}. In log analysis using Transformer-based models, attention visualization can show which tokens or templates receive high attention weights during anomaly detection tasks \cite{SoKBhusal_2023}.  The use of attention weights as explanations is controversial, with researchers debating their faithfulness and stability \cite{SoKBhusal_2023}. The argument that "Attention is not Explanation" \cite{AttentionNotExplanation} claims that if alternative sets of attention weights can be found that produce near-identical model predictions, then the original attention distribution cannot be the exclusive, faithful explanation for that prediction \cite{AttentionNotExplanation}. If the goal is to achieve explainability by providing a reasonable justification for a model’s prediction, the presence of alternative explanations does not necessarily reduce the value of the original one.

\subsubsection{Brief Overview of LIME}

LIME (Local Interpretable Model-agnostic Explanations) provides explanations for individual model predictions by learning a simple, interpretable surrogate model locally around the specific prediction instance \cite{WhyShouldITrustYou}. The core process involves perturbing the input instance to generate new data samples, obtaining predictions from the black-box model for these perturbed samples, weighting these samples by their proximity to the original instance, and training a simple, interpretable model (often sparse linear regression) on the weighted samples \cite{WhyShouldITrustYou, XAImersha2024}. The weights of this local surrogate model serve as the explanation, indicating the degree to which each input feature contributes to the prediction \cite{WhyShouldITrustYou}. LIME aims to minimize a function that balances local fidelity (how accurately the simple model approximates the black-box prediction locally) and the simplicity of the explanation \cite{XAImersha2024}. While LIME is efficient, its explanations can sometimes be unreliable or susceptible to adversarial manipulation, partly because it assumes feature independence \cite{WhyShouldITrustYou}.

\begin{table}[h!]
\centering
\small
\begin{tabular}{|l|p{5.5cm}|p{5.5cm}|}
\hline
\textbf{Feature} & \textbf{LIME} & \textbf{SHAP} \\
\hline
Methodology & Perturbation-based local approximation. & Game-theoretic (Shapley values); additive attribution. \\
Scope & Local explanation for a single instance. & Local explanation, but derived from a framework designed for global coherence. \\
Model Dependency & Model-agnostic. & Model-agnostic (using approximations). \\
Theoretical Basis & Minimizes local fidelity and complexity loss. & Unique solution satisfying local accuracy, missingness, and consistency. \\
Computational Cost & Generally faster for local explanations. & Computationally slower (though approximations exist). \\
\hline
\end{tabular}
\caption{Comparison of LIME and SHAP methodologies.}
\end{table}

\subsubsection{Application to Security Domains}

XAI techniques are applied in security domains such as anomaly detection using system logs. Modern log anomaly detection models like LogFiT and LAnoBERT often use fine-tuned, pre-trained BERT-based language models to learn the linguistic structure and sequential patterns of normal log data \cite{LAnoBERT, LogFiT}. In this context, explanations serve to validate alerts and understand why a sequence of logs was flagged as anomalous. LIME and SHAP are used to assign relevance scores to individual log events within an anomalous sequence, helping analysts evaluate model-generated alarms. DeepAID takes a contrastive approach by identifying the nearest normal sample corresponding to the anomalous input, highlighting the specific event that caused the anomalous prediction by showing differences between the anomaly and the reference normal log \cite{DeepAID}. In hybrid systems, specialized models extract attention weights from Transformer layers to visualize which log tokens or sequences were most critical to the anomaly prediction \cite{MLAD}.

HuntGPT, an intrusion detection dashboard, integrates a Random Forest classifier with SHAP and LIME to explain predictions, coupled with a GPT-3.5 Turbo conversational agent to deliver insights in natural language format \cite{ali2025}. Similarly, the AnomalyExplainerBot framework uses conversational AI along with visualization tools like BERTViz and Captum (which provides feature attribution) to explain LLM-based anomaly detection decisions on log data \cite{AnomalyExplainer}. 

