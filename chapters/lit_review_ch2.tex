\section{Background}\label{chapter:lit-review}



\subsection{Introduction to Log Anomaly Detection}



\subsubsection{Overview and Challenges}





The information within system log files is fundamental to monitoring the stability and security of networked systems, which generate these logs ubiquitously \cite{almodovar-etal-2022-language, DeepLog}. Logs provide this value by offering a detailed, chronological record of both runtime system events and user intentions \cite{LogFormer}. Consequently, as system complexity grows, these logs have become a critical asset for operations such as performance monitoring, security auditing, transaction tracing, and fault diagnosis \cite{LogCTBL}.



The primary purpose of log anomaly detection is to protect digital infrastructures by identifying abnormal activities, such as network intrusions, from the enormous volumes of event logs. In this context, anomalies represent log patterns that significantly deviate from the expected behavior of the system. Detecting these deviations is crucial for maintaining system reliability and preventing severe disruptions or financial losses, as global cybercrime costs are estimated to reach trillions of euros annually \cite{europeanparliament2024cybercrime}.



Log analysis, and consequently anomaly detection, faces several significant challenges primarily driven by the nature and scale of the data:



\begin{itemize}

     \item \textbf{Volume:} System logs are large-scale data collected in real-time \cite{LAnoBERT}. The sheer volume of logs has grown rapidly, often reaching 50 GB (120â€“200 million lines) per hour for large-scale services, making manual inspection and traditional processing infeasible \cite{LogFormer}.



    \item \textbf{Variety and Complexity:} Logs are typically unstructured or semi-structured text files generated by logging statements in source code \cite{DeepLog}. Because developers are allowed to write free-text messages, the format and semantics of logs vary significantly across systems, leading to high-dimensional features with complex interrelationships. This complexity and diversity increase the difficulty of accurate anomaly detection \cite{Drain}.



    \item \textbf{Velocity (Timeliness):} For anomaly detection to be useful, it must be timely, requiring decisions to be made in a streaming fashion to allow users to intervene in ongoing attacks or performance issues. Offline methods that require multiple passes over the entire log data are thus unsuitable for real-time security monitoring \cite{DeepLog}.

    

\end{itemize}



Due to the challenges of volume and complexity, 

the adoption of automated log analysis has become imperative to efficiently process and interpret vast corpora of logs.



\subsubsection{Traditional Methods}

Early log anomaly detection efforts relied heavily on human expertise \cite{LogCTBL}. As the volume of logs grew, research shifted toward automated, data-driven methods, broadly categorized into rule-based systems and statistical approaches, many of which depend on logs first being converted into a structured format through a process known as log parsing \cite{Drain}.



Log parsing is a critical precursor step where raw, unstructured log messages are transformed into structured data, typically by extracting a constant part, called the log template (or log key), and identifying the variable parts (parameters).

The parser Spell, for example, is an online streaming parser that utilizes the Longest Common Subsequence (LCS) technique to dynamically identify and update log patterns. Tools like DeepLog rely on log parsing methods like Spell to generate log templates for their inputs \cite{DeepLog, Spell}.



\paragraph{Rule-Based Systems (Regex, Signatures)}

Rule-based methodologies were among the first attempts to automate log analysis to reduce human error. These methods typically rely on explicitly defined rules, patterns, or known indicators of abnormal behavior, often requiring specific domain knowledge from human experts.



\begin{itemize}

    \item \textbf{Keyword Matching and Regular Expressions (Regex):} Early rule-based systems focused on matching specific keywords (e.g., "error," "failed") or using regular expressions to flag anomalous log entries \cite{robust}. However, relying solely on keywords or structural features often prevents a large portion of log anomalies from being detected and can lead to unnecessary alarms (alarm fatigue) if the system constantly evolves \cite{robust,LogAnomaly}. Furthermore, manually designing and maintaining regular expressions is prohibitive given the rapid increase in log volume and frequent system updates \cite{Drain}.



    \item \textbf{Invariant Mining (IM):} Invariant mining is another traditional approach that captures co-occurrence patterns between different log keys \cite{DeepLog,LogAnomaly}. This method defines a window (time or session based) and detects whether certain mined quantitative relationships, or invariants, hold true within that window (e.g., ensuring that the count of "file open" logs equals the count of "file close" logs in a normal condition) \cite{LogAnomaly}. IM is typically characterized as an unsupervised offline method \cite{DeepLog}.

\end{itemize}



\paragraph{Statistical Methods (Clustering, PCA)}

Statistical methods leverage mathematical principles to identify normal patterns from data volumes and flag deviations statistically likely to be anomalous. These methods generally operate on a generated numeric vector representation of the logs, often discarding parameter values and only using log keys and their counts \cite{DeepLog}. Most statistical methods rely on initial log parsing, where raw log messages are converted into structured, numeric representations, such as event count vectors \cite{SystemLog}.





\subparagraph{Principal Component Analysis (PCA):} PCA is a widely used statistical technique in log anomaly detection. PCA is a linear transformation technique used to transform a set of correlated variables into a set of uncorrelated variables, known as principal components \cite{LAnoBERT, geron2019hands}.

    

    Application in Log Analysis:



    \begin{itemize}

        \item \textbf{Vectorization:} PCA is applied at the session level, where log entries are grouped by an identifier (e.g., block\_id in HDFS or instance\_id in OpenStack) \cite{SystemLog, DeepLog}. Each session is converted into an event count vector that records how often each unique log key occurs. These vectors together form a feature matrix, with rows as sessions and columns as log keys \cite{SystemLog, DeepLog}.



        \item \textbf{Dimensionality Reduction:} PCA projects the high-dimensional counting matrix into a lower-dimensional space by identifying components that capture the most variance \cite{LogCTBL}. It produces two subspaces: the normal space ($S_n$), formed by the first $k$ principal components, and the anomaly space ($S_a$), composed of the remaining ones \cite{SystemLog}.



        \item \textbf{Anomaly Detection:} PCA generates a normal space ($S_n$) using the first $k$ principal components and an anomaly space ($S_a$) using the remaining dimensions \cite{SystemLog}. An abnormal session is detected by measuring its projection length (quantified by the Squared Prediction Error, SPE) onto this residual subspace $S_a$ \cite{SystemLog, robust, DeepLog}.



    \end{itemize}



\subparagraph{Clustering Methods}



 Clustering algorithms aim to group data instances that are similar to each other. In log analysis, these methods are used to group log messages to generate event templates (log parsing) or to detect anomalies by identifying outliers that do not belong to normal clusters \cite{alammar2024hands, geron2019hands}.



\begin{itemize}

    \item \textbf{LogCluster:} LogCluster is an unsupervised clustering method that groups textually similar log messages to detect frequent line patterns and abnormal events in textual logs \cite{LogAnomaly,LAnoBERT}. It vectorizes log sequences using Inverse Document Frequency (IDF) scaling and builds a knowledge base of normal and abnormal clusters \cite{SystemLog,LogCTBL}. A new sequence is classified as an anomaly if its distance to the nearest cluster centroid exceeds a threshold \cite{LogCTBL}. 



    \item \textbf{Density-Based Methods:} The Simple Logfile Clustering Tool (SLCT) uses a density-based approach to group log messages that occur frequently, forming clusters that represent common line patterns \cite{LogHound}. Log entries that do not fit into these clusters are considered outliers and may indicate rare or abnormal events \cite{LogHound}. Related methods, such as HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise), are also used in some cases to provide pseudo-labels for unlabeled data in semi-supervised learning \cite{LogCTBL}.  



\end{itemize}



\subsubsection{Machine Learning Approaches}



Following traditional statistical and rule-based methods, log anomaly detection quickly adopted general Machine Learning (ML) algorithms. These approaches rely heavily on the preceding log parsing and vectorization steps to transform unstructured log files into numerical features \cite{almodovar-etal-2022-language}. Traditional ML techniques are generally categorized into supervised models, which require labeled data, and unsupervised models, which are crucial given that log data is overwhelmingly unlabeled in real-world scenarios \cite{robust,LAnoBERT}.



\paragraph{Classification Models (SVM, Random Forest)}

Supervised classification models require labeled log data that indicate normal and anomalous behavior \cite{alammar2024hands}.



\begin{itemize}

    \item \textbf{Support Vector Machines (SVM):} SVM is a flexible model capable of linear or nonlinear classification, regression, and outlier detection. For log analysis, event count matrices are commonly used as input. SVMs can be adapted for multi-class problems using strategies like one-versus-the-rest \cite{geron2019hands}. The \textbf{One-Class SVM (OCSVM)} is an unsupervised variant trained only on normal data to identify anomalies that fall outside a learned boundary \cite{oneclasssvm}.



    \item \textbf{Random Forests (RF):} Random Forest is an ensemble method that builds multiple decision trees on random subsets of features and data, then combines their predictions \cite{Ko2025}. RF models are effective with minimal parameter tuning and do not require input scaling \cite{muller2016introduction}. In log anomaly detection, Random Forest often outperforms other classifiers and can be integrated into systems like \textbf{HuntGPT} \cite{ali2025} alongside Explainable AI techniques for enhanced threat analysis.

\end{itemize}



\paragraph{Unsupervised Methods (Isolation Forest)}

Unsupervised methods are critical for log analysis because acquiring labeled data is expensive, and most real-world log data is inherently unlabeled \cite{robust,LAnoBERT}.



\begin{itemize}

    \item \textbf{Isolation Forest (iForest):} iForest is a tree-based unsupervised learning algorithm specifically designed for anomaly detection \cite{IsolationForest}. Unlike clustering methods that seek dense regions, iForest focuses on isolating observations that are distinct from the remaining input data \cite{LAnoBERT,geron2019hands}. This is achieved by forming an ensemble of decision trees that partition the data; outliers are isolated more quickly (with fewer partitions) than normal instances \cite{LAnoBERT}. 



\end{itemize}



\subsubsection{Deep Learning Models}



Deep Learning (DL) approaches use neural networks to automatically extract features and capture complex patterns from logs, offering advantages for high-dimensional and large-scale data \cite{LogCTBL}. DL methods usually follow a pipeline of preprocessing, parsing, vectorization, and neural network classification \cite{hashemi2025}.



\paragraph{LSTM-based Models}

Recurrent Neural Networks (RNNs), especially with \textbf{Long Short-Term Memory (LSTM)} units, are widely used for sequential log data analysis \cite{robust}.



\begin{itemize}

    \item \textbf{DeepLog :} DeepLog is an early deep learning framework for online log anomaly detection. It treats logs as sequences similar to natural language and uses an LSTM trained on log keys derived from parsing tools like Spell \cite{DeepLog, Spell}. The model predicts the next log key in a sequence to learn normal patterns, and any log key with low prediction probability is flagged as anomalous \cite{DeepLog}. Detection occurs at the level of individual log entries, and the model can incorporate parameter values and time intervals to identify performance-related anomalies \cite{almodovar-etal-2022-language,DeepLog}.



    

    \item \textbf{LogAnomaly and LogRobust:} LogAnomaly combines LSTM with \textit{template2vec} embeddings to capture sequential and quantitative anomalies. LogRobust uses an attention-based Bi-LSTM with Word2Vec representations to capture bidirectional and semantic relationships among log events.

\end{itemize}



\paragraph{Transformer-based Models}

Transformer architectures, originally from NLP, capture long-range dependencies and semantic context in logs.



\begin{itemize}

    \item \textbf{LogBERT:} Applies the BERT architecture to encode log messages into semantic embeddings. These embeddings are then used by a Transformer-based classifier to detect anomalies. Similar approaches, such as \textbf{NeuralLog}, bypass traditional parsing and work directly on raw logs.

    

    \item \textbf{LogFiT:} Fine-tunes pre-trained BERT models on system logs using self-supervised objectives like Masked Token Prediction combined with centroid distance minimization.

\end{itemize}



\paragraph{Attention Mechanisms}

Attention mechanisms enable models to prioritize the importance of different log entries, thereby enhancing feature extraction for anomaly detection.



\begin{itemize}

    \item \textbf{Multi-Head Attention:} Enables the model to track multiple patterns simultaneously.

    \item Attention is used in Bi-LSTM models like LogRobust and hybrid architectures like \textbf{LogCTBL} (CNN-TCN-Bi-LSTM + BERT) to enhance feature representation.

    \item Attention scores can also aid Explainable AI (XAI) by highlighting which log events contribute most to anomaly predictions.

\end{itemize}



\paragraph{Performance}

Deep learning models typically achieve high F1 scores based on their ability to model sequential and semantic patterns. Representative results include:



\begin{table}[h!]

\centering

\begin{tabular}{|l|l|l|l|}

\hline

\textbf{Model} & \textbf{Architecture} & \textbf{Dataset} & \textbf{F1 Score} \\

\hline

LogCTBL & Hybrid (CNN-TCN-Bi-LSTM + BERT) & BGL & 0.9987 \\

LogCTBL & Hybrid (CNN-TCN-Bi-LSTM + BERT) & Thunderbird & 0.9978 \\

SiaLog & Siamese Network (LSTM) & HDFS & 0.99 \\

SiaLog & Siamese Network (LSTM) & BGL & 0.99 \\

OneLog & HCNN (Character-based) & HDFS & 0.99 \\

OneLog & HCNN (Character-based) & BGL & 0.99 \\

DeepLog & LSTM & HDFS & 0.85 \\

DeepLog & LSTM & BGL & 0.86 \\

LogAnomaly & LSTM + Template2Vec & BGL & 0.88 \\

LogBERT & Transformer (BERT) & BGL & 0.91 \\

\hline

\end{tabular}

\caption{Comparison of deep learning log anomaly detection models.}

\end{table}